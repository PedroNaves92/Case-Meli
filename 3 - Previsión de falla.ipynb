{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas relevantes para o projeto\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support as score, roc_curve, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, cross_validate, RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "Importando o dataframe e explorando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando o csv com o dataframe\n",
    "#Corrigido o encode para ANSI\n",
    "#Feito o parse da coluna 'date' para formato datetime do pandas\n",
    "dfBase = pd.read_csv('DataSets/full_devices.csv', encoding='ANSI', parse_dates=['date'])\n",
    "dfBase.info()\n",
    "dfBase.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nenhum campo vazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checando campos vazios\n",
    "dfBase.isnull().sum(axis = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma data e device com o registro duplicado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checando par de data e device duplicados\n",
    "dfBase[dfBase.duplicated(subset=['date','device'])==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estatísticas descritivas do df\n",
    "dfBase.describe()\n",
    "\n",
    "#attribute 7 e 8 tem exatamente a mesma estatística descritiva, pode ser uma coluna duplicada"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os attribute 7 e attribute 8 são provavelmente atributos iguais duplicados na base, o heatmap ajuda a perceber isso.\n",
    "\n",
    "Além disso, há uma possível correlação entre o atributte 3 e o 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigando possíveis correlações entre as features\n",
    "sns.heatmap(dfBase.corr(), annot=True, fmt=\".1f\", linewidth=.5, cmap='RdBu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "Tratamento de dados\n",
    "\n",
    "- Removendo o registro duplicado\n",
    "- Removendo a feature duplicada\n",
    "- Mantive as features 3 e 9. 0.5 não foi correlação suficiente para removê-las.\n",
    "- Em um cenário real, onde conheço as features e sei o que cada uma representa, poderia ainda incluir aqui um passo de feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpando problemas encontrados\n",
    "dfLimpo = dfBase.drop_duplicates(subset=['date','device']) #Removendo duplicatas\n",
    "dfLimpo = dfLimpo.drop(columns='attribute8') #Removendo a feature duplicada 'attribute8'\n",
    "print(dfLimpo.shape)\n",
    "dfLimpo.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "Com os dados limpos é hora de separar as variáveis preditores e as de resposta, dividir também nosso dataset em teste e treino com estratificação.\n",
    "\n",
    "Como os dados estão bem desbalanceados nas classes, utilizei um método de oversampling para balancear melhor, igualando a proporção de falha com não falha.\n",
    "\n",
    "Escolhi alguns modelos e validei a acurácia deles com uma metodologia de Kfold.\n",
    "\n",
    "Random_State 42 é a resposta do universo pra tudo. haha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sepração de X e Y e definição do oversample\n",
    "x = dfLimpo.drop(columns=['date','device','failure'])\n",
    "y = dfLimpo['failure']\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regressão logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regressão logística\n",
    "modelo = LogisticRegression(random_state=42,class_weight='balanced')\n",
    "\n",
    "steps = [('over', oversample), ('modelo', modelo)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(pipeline, x, y, scoring='f1_micro', cv=cv)\n",
    "scores.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Árvore de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Árvore de decisão\n",
    "modelo = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "steps = [('over', oversample), ('modelo', modelo)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(pipeline, x, y, scoring='f1_micro', cv=cv)\n",
    "scores.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "modelo = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
    "\n",
    "steps = [('over', oversample), ('modelo', modelo)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(pipeline, x, y, scoring='f1_micro', cv=cv)\n",
    "scores.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive_Bayes\n",
    "modelo = GaussianNB()\n",
    "\n",
    "steps = [('over', oversample), ('modelo', modelo)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(pipeline, x, y, scoring='f1_micro', cv=cv)\n",
    "scores.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "Tratamento final dos dados para modelo escolhido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sepração de X e Y e definição do oversample\n",
    "x = dfLimpo.drop(columns=['date','device','failure'])\n",
    "y = dfLimpo['failure']\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "x_over, y_over = oversample.fit_resample(x,y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_over,y_over,train_size=0.7, random_state=42)\n",
    "\n",
    "\n",
    "print('train: ', x_train.shape, y_train.shape)\n",
    "print(y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo = LogisticRegression(random_state=42)\n",
    "#modelo = DecisionTreeClassifier(random_state=42)\n",
    "modelo = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
    "#modelo = GaussianNB()\n",
    "modelo.fit(x_train, y_train)\n",
    "\n",
    "y_pred = modelo.predict(x_test)\n",
    "y_prob = pd.DataFrame(modelo.predict_proba(x_test))\n",
    "\n",
    "metricas = classification_report(y_test,y_pred)\n",
    "print(metricas)\n",
    "print(modelo.score(x_test,y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "Matriz de confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPredicts = x_test.copy()\n",
    "dfPredicts = dfPredicts.reset_index(drop=True)\n",
    "dfPredicts['y real'] = pd.DataFrame(y_test).reset_index(drop=True)\n",
    "dfPredicts['y pred'] = pd.DataFrame(y_pred).reset_index(drop=True)\n",
    "dfPredicts['y prob'] = pd.DataFrame(y_prob[1]).reset_index(drop=True)\n",
    "dfPredicts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
